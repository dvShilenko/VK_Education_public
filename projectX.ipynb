{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9b9f8809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selectolax in /Users/dmitryshilenko/opt/anaconda3/lib/python3.9/site-packages (0.3.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selectolax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5477ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: navec in /Users/dmitryshilenko/opt/anaconda3/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy in /Users/dmitryshilenko/opt/anaconda3/lib/python3.9/site-packages (from navec) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install navec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "449318d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dmitryshilenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dmitryshilenko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import lxml.html\n",
    "from selectolax.parser import HTMLParser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from navec import Navec\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "navec_path = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
    "navec = Navec.load(navec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97880a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train_groups.csv')\n",
    "X = np.reshape(np.array([0] * 37), (1, 37))\n",
    "for i in np.unique(df['group_id']):\n",
    "    print(i)\n",
    "    inds = df[df['group_id'] == i]['doc_id']\n",
    "    temp_text = []\n",
    "    for j in inds:\n",
    "        temp_text.append(get_text('content', j))\n",
    "    temp_tkns = []\n",
    "    embeddings = []\n",
    "    for x in temp_text:\n",
    "        t = tokenize(x)\n",
    "        temp_tkns.append(t)\n",
    "        embeddings.append(doc_2_vec(t))\n",
    "    mean_embds = []\n",
    "    for x in embeddings:\n",
    "        mean_embds.append(np.mean(x, axis=0))\n",
    "    kmeans_feature = KMeans_dist(mean_embds)\n",
    "    dbscan_f = dbscan_feature(mean_embds)\n",
    "    dev_feature = deviation(mean_embds)\n",
    "    norm_feature = embed_norm(mean_embds)\n",
    "    top15_feature = top15(temp_tkns)\n",
    "    top15_binary_feature = top15_feature[0][0]\n",
    "    top15_float_feature = top15_feature[0][1]\n",
    "    for i in range(1, len(top15_feature)):\n",
    "        if len(top15_feature[i][0]) != 15:\n",
    "            top15_binary_feature = np.vstack((top15_binary_feature, [0]*15))\n",
    "            top15_float_feature = np.vstack((top15_float_feature, [0]*15))\n",
    "        else:\n",
    "            top15_binary_feature = np.vstack((top15_binary_feature, top15_feature[i][0]))\n",
    "            top15_float_feature = np.vstack((top15_float_feature, top15_feature[i][1]))\n",
    "    sub_set = np.hstack((np.reshape(kmeans_feature, (kmeans_feature.shape[0], 1)), np.reshape(dbscan_f, (dbscan_f.shape[0], 1)), np.reshape(dev_feature, (dev_feature.shape[0], 1)), np.reshape(norm_feature, (norm_feature.shape[0], 1)), top15_binary_feature, top15_float_feature))\n",
    "    svm_feature = SVM_fit(sub_set)\n",
    "    sub_set = np.hstack((sub_set, np.reshape(svm_feature, (svm_feature.shape[0], 1))))\n",
    "    outlier_feature = feature_local_outlier(sub_set)\n",
    "    sub_set = np.hstack((sub_set, np.reshape(outlier_feature, (outlier_feature.shape[0], 1))))\n",
    "    isolation_feature = iso_forest(sub_set)\n",
    "    sub_set = np.hstack((sub_set, np.reshape(isolation_feature, (isolation_feature.shape[0], 1))))\n",
    "    X = np.vstack((X, sub_set))\n",
    "X = X[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3b864309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test_groups.csv')\n",
    "X_test = np.reshape(np.array([0] * 37), (1, 37))\n",
    "for i in np.unique(df['group_id']):\n",
    "    print(i)\n",
    "    inds = df[df['group_id'] == i]['doc_id']\n",
    "    temp_text = []\n",
    "    for j in inds:\n",
    "        temp_text.append(get_text('content', j))\n",
    "    temp_tkns = []\n",
    "    embeddings = []\n",
    "    for x in temp_text:\n",
    "        t = tokenize(x)\n",
    "        temp_tkns.append(t)\n",
    "        embeddings.append(doc_2_vec(t))\n",
    "    mean_embds = []\n",
    "    for x in embeddings:\n",
    "        mean_embds.append(np.mean(x, axis=0))\n",
    "    kmeans_feature = KMeans_dist(mean_embds)\n",
    "    dbscan_f = dbscan_feature(mean_embds)\n",
    "    dev_feature = deviation(mean_embds)\n",
    "    norm_feature = embed_norm(mean_embds)\n",
    "    top15_feature = top15(temp_tkns)\n",
    "    top15_binary_feature = top15_feature[0][0]\n",
    "    top15_float_feature = top15_feature[0][1]\n",
    "    for i in range(1, len(top15_feature)):\n",
    "        if len(top15_feature[i][0]) != 15:\n",
    "            top15_binary_feature = np.vstack((top15_binary_feature, [0]*15))\n",
    "            top15_float_feature = np.vstack((top15_float_feature, [0]*15))\n",
    "        else:\n",
    "            top15_binary_feature = np.vstack((top15_binary_feature, top15_feature[i][0]))\n",
    "            top15_float_feature = np.vstack((top15_float_feature, top15_feature[i][1]))\n",
    "    sub_set = np.hstack((np.reshape(kmeans_feature, (kmeans_feature.shape[0], 1)), np.reshape(dbscan_f, (dbscan_f.shape[0], 1)), np.reshape(dev_feature, (dev_feature.shape[0], 1)), np.reshape(norm_feature, (norm_feature.shape[0], 1)), top15_binary_feature, top15_float_feature))\n",
    "    svm_feature = SVM_fit(sub_set)\n",
    "    sub_set = np.hstack((sub_set, np.reshape(svm_feature, (svm_feature.shape[0], 1))))\n",
    "    outlier_feature = feature_local_outlier(sub_set)\n",
    "    sub_set = np.hstack((sub_set, np.reshape(outlier_feature, (outlier_feature.shape[0], 1))))\n",
    "    isolation_feature = iso_forest(sub_set)\n",
    "    sub_set = np.hstack((sub_set, np.reshape(isolation_feature, (isolation_feature.shape[0], 1))))\n",
    "    X_test = np.vstack((X_test, sub_set))\n",
    "X_test = X_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f7ec7482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score = 0.6323987538940811\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y = df_train['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "bst = XGBClassifier(n_estimators=401, learning_rate=0.5)\n",
    "bst.fit(X_train, y_train)\n",
    "y_pred = bst.predict(X_test)\n",
    "f_score = f1_score(y_test, y_pred)\n",
    "print(f'F-1 score = {f_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9c645113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score with 0.4 rate = 0.6168674698795181\n",
      "F-1 score with 0.42000000000000004 rate = 0.6178861788617888\n",
      "F-1 score with 0.44 rate = 0.6142742582197274\n",
      "F-1 score with 0.46 rate = 0.6245954692556634\n",
      "F-1 score with 0.48000000000000004 rate = 0.6198547215496367\n",
      "F-1 score with 0.5 rate = 0.6368078175895765\n",
      "F-1 score with 0.52 rate = 0.619281045751634\n",
      "F-1 score with 0.54 rate = 0.6166394779771615\n",
      "F-1 score with 0.56 rate = 0.611111111111111\n",
      "F-1 score with 0.5800000000000001 rate = 0.6060113728675874\n",
      "F-1 score with 0.6000000000000001 rate = 0.626311541565779\n"
     ]
    }
   ],
   "source": [
    "for x in [0.4 + 0.02 * i for i in range(11)]:\n",
    "    bst = XGBClassifier(n_estimators=401, learning_rate=x)\n",
    "    bst.fit(X_train, y_train)\n",
    "    y_pred = bst.predict(X_test)\n",
    "    f_score = f1_score(y_test, y_pred)\n",
    "    print(f'F-1 score with {x} rate = {f_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b0cd123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9e48c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(y_test)\n",
    "predict = predict.reset_index()\n",
    "predict.columns = ['pair_id', 'target']\n",
    "predict['pair_id'] += 11691\n",
    "predict.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7e4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(path: str, doc_id: int):\n",
    "    f = open(path + '/' + str(doc_id) + '.dat', 'r')\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1024ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ru = nltk.corpus.stopwords.words('russian')\n",
    "stopwords_en = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "stopwords_ru.append('это')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def tokenize(doc):\n",
    "    tokens = [i.lower() for i in tokenizer.tokenize(doc) if i.lower() not in stopwords_ru and i.isdigit() == False and len(i) > 1 and i.lower() not in stopwords_en]\n",
    "    stems = [stemmer.stem(i) for i in tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0283c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_dist(X:np.array):\n",
    "    centre = np.mean(X, axis=0)\n",
    "    all_dist = np.sum(np.square(X - centre), axis=1)\n",
    "    return all_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e166942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_common_words(X):\n",
    "    arr = np.zeros((len(X), len(X)))\n",
    "    for i in range(len(X)):\n",
    "        for j in range(i, len(X)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            else:\n",
    "                intersect = len(set(tokenize(X[i])).intersection(set(tokenize(X[j]))))\n",
    "                arr[i, j] = intersect\n",
    "                arr[j, i] = intersect\n",
    "    return np.sum(arr, axis=1) / (len(X) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b6e58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_2_vec(tokens):\n",
    "    res = []\n",
    "    for x in tokens:\n",
    "        if x in navec:\n",
    "            res.append(navec[x])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "501782b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_feature(X):\n",
    "    clustering = DBSCAN(eps=1).fit(X)\n",
    "    pred = clustering.labels_\n",
    "    cast = lambda x: 1 if x == -1 else 0\n",
    "    vec_cast = np.vectorize(cast)\n",
    "    pred = vec_cast(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6df52b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top15(X): # X - токены для каждого документа\n",
    "    total_top = {}\n",
    "    every_docs_top = {}\n",
    "    int_docs_top = []\n",
    "    j = 0\n",
    "    for i in X:\n",
    "        tokens, counts = np.unique(i, return_counts=True)\n",
    "        counts = counts / len(i)\n",
    "        sorted_counts = np.argsort(counts)[-15:]\n",
    "        doc_top_15 = (tokens[sorted_counts], counts[sorted_counts])\n",
    "        every_docs_top[j] = doc_top_15\n",
    "        j += 1\n",
    "        for k in sorted_counts:\n",
    "            if tokens[k] in total_top:\n",
    "                total_top[tokens[k]] += counts[k]\n",
    "            else:\n",
    "                total_top[tokens[k]] = counts[k]\n",
    "    total_top = dict(sorted(total_top.items(), key=lambda x: x[1], reverse=True)[:15])\n",
    "    every_docs_top = list((list(every_docs_top.values())))\n",
    "    for i in every_docs_top:\n",
    "        temp_arr = []\n",
    "        for j in range(len(i[0])):\n",
    "            if i[0][j] in total_top:\n",
    "                temp_arr.append(1)\n",
    "            else:\n",
    "                temp_arr.append(0)\n",
    "        int_docs_top.append((temp_arr, i[1]))\n",
    "    return int_docs_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca984c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deviation(X):\n",
    "    mean_vec = np.mean(X, axis=0)\n",
    "    dev_matr = X - mean_vec\n",
    "    return np.linalg.norm(dev_matr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b64c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_norm(X):\n",
    "    return np.linalg.norm(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7b7ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_fit(X):\n",
    "    svm = OneClassSVM(nu = 0.01, kernel = 'rbf', gamma = 'auto').fit(X)\n",
    "    return svm.score_samples(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da9945b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_local_outlier(X):\n",
    "    clf = LocalOutlierFactor()\n",
    "    clf.fit(X)\n",
    "    return clf.negative_outlier_factor_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dfed297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_forest(X):\n",
    "    clf = IsolationForest().fit(X)\n",
    "    return clf.score_samples(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_fit(X):\n",
    "    svm = OneClassSVM(nu = 0.01, kernel = 'rbf', gamma = 'auto').fit(X) \n",
    "    return svm.score_samples(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
